{
  "hash": "d2cb81fcf318e8f6cc49d0a2fb1fd074",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Poisson Regression Examples\"\nauthor: \"Jaqueline Vallejo Hinojosa\"\ndate: today\ncallout-appearance: minimal # this hides the blue \"i\" icon on .callout-notes\n---\n\n\n\n\n\n## Blueprinty Case Study\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\n::: {#b93aa9c7 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd \n\nblueprinty = pd.read_csv('blueprinty.csv')\nblueprinty.head()\n\npatents_num = blueprinty.groupby('iscustomer')['patents'].mean()\npatents_num\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.histplot(data=blueprinty, x='patents', hue='iscustomer', multiple='stack', bins=20)\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Count')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\n* The histogram shows that non-customers consistently outnumber customers across patent counts, with most entities, regardless of customer status, tend to hold 3–4 patents.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n::: {#ba0bd2cf .cell execution_count=2}\n``` {.python .cell-code}\nregion_counts = blueprinty.groupby(['iscustomer', 'region']).size().unstack()\n#print(region_counts)\n\nregion_counts.T.plot(kind='bar', stacked=True)\nplt.title('Customer Status by Region')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.legend(title='Customer Status')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\n* The chart shows that the Northeast has the highest overall customer count and a relatively balanced customer-to-non-customer ratio, while all other regions are dominated by non-customers. \n\n::: {#9055c5e7 .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nblueprinty.boxplot(column='age', by='iscustomer')\nplt.title('Age Distribution by Customer Status')\nplt.suptitle('')\nplt.xlabel('Customer Status')\nplt.ylabel('Age')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\n* The age distribution for customers and non-customers is similar, with both groups centered around the mid-to-late 20s, though customers show slightly higher age variability and a marginally higher median\n\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nFor a random variable $Y \\sim \\text{Poisson}(\\lambda)$, we consider the probability mass function (PMF):\n\n$$\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n$$\n\nIf we observe **independent data points** $Y_1, Y_2, \\dots, Y_n$ from a Poisson distribution with the same rate parameter $\\lambda$, then the **likelihood function** is the product of their individual probabilities:\n\n$$\n\\mathcal{L}(\\lambda; Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\n$$\n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n$$\n\n::: {#b3cda41c .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lam, Y):\n    \"\"\"\n    Computes the log-likelihood of Poisson-distributed data.\n\n    Parameters:\n    lam : float\n        Poisson rate parameter (lambda)\n    Y : array-like\n        Observed counts\n\n    Returns:\n    float\n        Log-likelihood value\n    \"\"\"\n    Y = np.array(Y)\n    \n    if lam <= 0:\n        return -np.inf  # log-likelihood undefined for non-positive lambda\n\n    loglik = np.sum(-lam + Y * np.log(lam) - gammaln(Y + 1))\n    return loglik\n```\n:::\n\n\n::: {#15f6ae20 .cell execution_count=5}\n``` {.python .cell-code}\nY = blueprinty['patents'].values \n\nlambda_vals = np.linspace(0.1, 10, 200)\n\nloglik_vals = [poisson_loglikelihood(lam, Y) for lam in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, label='Log-Likelihood')\nplt.xlabel('Lambda (λ)')\nplt.ylabel('Log-Likelihood')\nplt.title('Poisson Log-Likelihood as a Function of λ')\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw2_questions_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\n::: {#680e8f1c .cell execution_count=6}\n``` {.python .cell-code}\nlambda_mle = Y.mean()\nprint(\"MLE of lambda (λ̂):\", lambda_mle)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMLE of lambda (λ̂): 3.6846666666666668\n```\n:::\n:::\n\n\n::: {#6cbf546c .cell execution_count=7}\n``` {.python .cell-code}\n#_todo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python._\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\n# Define the negative log-likelihood (for minimization)\ndef neg_poisson_loglikelihood(lam):\n    if lam <= 0:\n        return np.inf  \n    loglik = np.sum(-lam + Y * np.log(lam) - gammaln(Y + 1))\n    return -loglik  \ninitial_lambda = np.array([1.0])\n\nresult = minimize(neg_poisson_loglikelihood, initial_lambda, bounds=[(1e-6, None)])\n\nlambda_mle = result.x[0]\n\nprint(\"MLE of lambda (λ̂) from optimization:\", lambda_mle)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMLE of lambda (λ̂) from optimization: 3.6846668570379952\n```\n:::\n:::\n\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n::: {#5eb81b54 .cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    Log-likelihood function for Poisson regression with a log-link.\n    \n    Parameters:\n    beta : array-like\n        Coefficient vector (length p)\n    Y : array-like\n        Response vector (length n)\n    X : array-like\n        Covariate matrix (n x p)\n    \n    Returns:\n    float\n        Negative log-likelihood (for optimization)\n    \"\"\"\n    beta = np.array(beta, dtype=float)\n    Y = np.array(Y, dtype=float)\n    X = np.array(X, dtype=float)\n    \n    lambda_i = np.exp(X @ beta)  # log-link: lambda_i = exp(X_i * beta)\n    \n    loglik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n    \n    return -loglik  # Return negative log-likelihood for minimization\n```\n:::\n\n\n::: {#360c83fe .cell execution_count=9}\n``` {.python .cell-code}\n#_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._\n\n# import numpy as np\n# import pandas as pd\n# from scipy.optimize import minimize\n\n# Create age squared\nblueprinty['age_squared'] = blueprinty['age'] ** 2\n\n# One-hot encode 'region', excluding one level (reference group)\nX_region = pd.get_dummies(blueprinty['region'], drop_first=True)\n\n# Assemble the design matrix X\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name='intercept'),  # constant term\n    blueprinty['age'],\n    blueprinty['age_squared'],\n    X_region,\n    blueprinty['iscustomer']\n], axis=1)\n\nY = blueprinty['patents'].values\nX = X.values\n\n\n\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(poisson_regression_loglikelihood, initial_beta, args=(Y, X), method='BFGS')\n\nbeta_mle = result.x\nhessian_inv = result.hess_inv  # this is the estimated variance-covariance matrix\n\nfrom scipy.optimize import minimize\n\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(poisson_regression_loglikelihood, initial_beta, args=(Y, X), method='BFGS')\n\n# Extract coefficients and standard errors\nbeta_mle = result.x\nhessian_inv = result.hess_inv\nse_beta = np.sqrt(np.diag(hessian_inv))\nse_beta = np.sqrt(np.diag(hessian_inv))\n\n# Column names in same order as X\ncolumn_names = ['intercept', 'age', 'age_squared'] + list(X_region.columns) + ['iscustomer']\n\nresults_df = pd.DataFrame({\n    'Coefficient': beta_mle,\n    'Std. Error': se_beta\n}, index=column_names)\n\nprint(results_df)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_46616/51368662.py:24: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(X @ beta)  # log-link: lambda_i = exp(X_i * beta)\n/tmp/ipykernel_46616/51368662.py:26: RuntimeWarning: invalid value encountered in multiply\n  loglik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/tmp/ipykernel_46616/51368662.py:26: RuntimeWarning: invalid value encountered in add\n  loglik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/home/jovyan/Desktop/Spring/feature-env/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/tmp/ipykernel_46616/51368662.py:24: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(X @ beta)  # log-link: lambda_i = exp(X_i * beta)\n/tmp/ipykernel_46616/51368662.py:26: RuntimeWarning: invalid value encountered in multiply\n  loglik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/tmp/ipykernel_46616/51368662.py:26: RuntimeWarning: invalid value encountered in add\n  loglik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/home/jovyan/Desktop/Spring/feature-env/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/tmp/ipykernel_46616/51368662.py:24: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(X @ beta)  # log-link: lambda_i = exp(X_i * beta)\n/tmp/ipykernel_46616/51368662.py:26: RuntimeWarning: invalid value encountered in multiply\n  loglik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/tmp/ipykernel_46616/51368662.py:26: RuntimeWarning: invalid value encountered in add\n  loglik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/home/jovyan/Desktop/Spring/feature-env/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n             Coefficient  Std. Error\nintercept       1.480059         1.0\nage            38.016417         1.0\nage_squared  1033.539585         1.0\nNortheast       0.640979         1.0\nNorthwest       0.164288         1.0\nSouth           0.181562         1.0\nSouthwest       0.295497         1.0\niscustomer      0.553874         1.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_46616/51368662.py:24: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(X @ beta)  # log-link: lambda_i = exp(X_i * beta)\n/tmp/ipykernel_46616/51368662.py:26: RuntimeWarning: invalid value encountered in multiply\n  loglik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/tmp/ipykernel_46616/51368662.py:26: RuntimeWarning: invalid value encountered in add\n  loglik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/home/jovyan/Desktop/Spring/feature-env/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/tmp/ipykernel_46616/51368662.py:24: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(X @ beta)  # log-link: lambda_i = exp(X_i * beta)\n/tmp/ipykernel_46616/51368662.py:26: RuntimeWarning: invalid value encountered in multiply\n  loglik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n/tmp/ipykernel_46616/51368662.py:26: RuntimeWarning: invalid value encountered in add\n  loglik = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n```\n:::\n:::\n\n\n::: {#81a693e3 .cell execution_count=10}\n``` {.python .cell-code}\n#_todo: Check your results using R's glm() function or Python sm.GLM() function._\n```\n:::\n\n\n::: {#93b1ac27 .cell execution_count=11}\n``` {.python .cell-code}\n#_todo: Interpret the results._ \n```\n:::\n\n\n::: {#3b89aee4 .cell execution_count=12}\n``` {.python .cell-code}\n#_todo: What do you conclude about the effect of Blueprinty's software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences._\n```\n:::\n\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n::: {#1e999aff .cell execution_count=13}\n``` {.python .cell-code}\n#_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._\n```\n:::\n\n\n",
    "supporting": [
      "hw2_questions_files"
    ],
    "filters": [],
    "includes": {}
  }
}